TASK: Annotate HH-RLHF harmless-base preference pairs into OFNR schema.

Target Output File: Gemini3_pro_harmless_base_train.jsonl

Context:
You are processing the input file `hh-rlhf/harmless-base/train.jsonl`.
For EACH line in the input file, you must generate one corresponding JSONL object.

Output Handling:
1. Check if the file `Gemini3_pro_harmless_base_train.jsonl` exists.
2. If it does NOT exist, CREATE it.
3. If it DOES exist, APPEND the new JSONL line to it.

Input format (one line):
{
  "chosen": "Human: ...\nAssistant: ...",
  "rejected": "Human: ...\nAssistant: ..."
}

GOAL:
Produce strictly valid JSONL (one JSON object per line).
Do NOT wrap the output in a list or markdown block.
Each output line must conform to `ontologies/schema_ofnr.json`.

What to focus on:
- Harmlessness: safety alignment, de-escalation, refusal quality
- Translate pseudo-feelings into verified feelings
- Ensure observation is nonjudgmental (camera-test)
- Extract needs + actionable requests

How to fill schema:
- dataset.teacher_model = "gemini_3_pro"
- source.corpus = "hh-rlhf"
- source.folder = "harmless-base"
- source.split = "train" (or "test" if applicable)
- source.file = "train.jsonl"
- source.pair_type = "chosen_rejected"

input.format = "pair"
input.chosen = provided chosen string
input.rejected = provided rejected string
input.prompt = ONLY the last user turn extracted from the conversation

Safety:
- Always compute safety label
- If content contains self-harm, violence, wrongdoing â†’ disallowed with safe_redirect/refusal

Output format example:
{"id": "...", "ofnr": {...}}

REMEMBER: Output must be JSONL. No extra text.